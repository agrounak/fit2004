STRING = """Revise quick select, longest repeated substrings, proving by induction // Fibonacci base case is fib(0) = 0, fib(1) = 1 // Recursive algorithms can pre-create an array before calling //  Recursive quicksort is not inplace because space required for recursive call is not constant, is O(log N) because of the stack frames // Binary search invariant: key is in array[1...N] if and only if key in array[lo...hi-1] // Proving recurrence relations by induction: after proving base case, assume that inner term in recurrence step is true and convert recurrence step to the term // Quickselect: Call with k = N/2, Choose random pivot, partition it and if k == index(pivot), you found the median, if the k > index(pivot) then need to search RIGHT sublist, or else need to search left sublist // Worst-case of O(n^2) as it costs O(n^2) and the lists sizes keep halving so O(N^2 + N^2/2 + N^2/4) = O(N^2) // Longest repeated substring in suffix trie: Find deepest node with at least 2 children // 
"Binary search termination: lo = hi -1 because lo >= hi - 1 for while loop to terminate. Since lo < mid < hi -> lo < hi when while loop ends. So lo <= hi - 1 when while loops terminates. So based on two statements, loop ends when lo = hi - 1 // Counting sort with alphabets is O(N) because domain size is a constant // For comparison-based sorting algorithms, still need to consider word size //
"
Revise suffix arrays // Remember to write constants a,b in recurrence relation // Right-right case for AVL is when node has BF of -2 and right child has BF of 0 or less // Merge sort aux space is O(n) and quick sort aux space is O(log n) // To make suffix array -> get all suffixes (including $). create rank array -> sort on 1st,2nd,4th,...,N/2,N characters,  if current ranks are same, suffix with smaller rank is smaller. if current ranks are the same, first k characters must be the same // Suffix array substring search: do binary search // Suffix array construction is now O(N log^2 N) and will take O(N) space // 
Revise suffix trees // 
Quick select, remember to specify N//2 to find the median of the list // Recurrence relations: When doing a single loop, need to specify a*N instead of just N because there is a constant operation being done // Suffix tree: Longest repeated substring involves finding the deepest (so you can find the longest) node with at least 2 children //
"sum(i=0, n) r^i = r^(n+1) -1 / r - 1 // Remember b^logb(k) = k //
"

Quicksort depends on two main factors: partition algorithm and pivot selection //
Top-down DP is basically bottom-up DP but when assigning to a past value, you instead call the DP method to get that value // 

Cuckoo hashing: Two tables with two hash functions, when inserting it kicks out w/e was there and that item has to find a new home, keeps going // AVL trees: Whoever becomes the head honcho says 'take this L' to the previous head honcho // Double hashing found by : Index = (hash1(key) + i*hash2(key)) % M //
"Suffix tree is a compact suffix trie (compressed to (index, length) as the node) // Prefix doubling for suffix arrays reduce construction cost to O(N log^2 N) (each sorting takes O(N log N) and you sort 1,2,..,N/2, N characters) //
"
"Naive method for BWT inversion: Sort BWT -> Do BWT + result -> Sort it -> Do BWT + result -> ... -> Get first row // Efficient method: Relies on fact that relative orders of letters in first column is same with the last column //
"
Graph is sparse if E << V^2 // Time complexity of checking if edge exists in graph using adjacency list is O(log V) if you do binary search, O(V) without // BFS involves adding src to discovered, for every adjacent edge, add undiscovered vertices to discovered, then put the vertex in finalised, keep going // BFS and DFS complexity is O(V+E), visits every vertex once and every edge at max twice // DFS, start at src, mark vertex as visited, for each edge call DFS(vertex) // Can do single source all targets problem with BFS, Dijkstra's algorithm, Bellman-Ford algorithm // BFS (shortest path): Start with src, for each outgoing edge, if not discovered/finalised u.distance = v.distance + 1, add to discovered // Dijkstras: Visit each edge once O(E) and while loop runs for O(V) times, searching for vertex with smallest distance O(V) so total O(E+V^2) = O(V^2) // Dijkstra's algorithm consists of finding smallest in discovered AND updating a given entry's distance // Prove disjkstra's with proof by contradiction // Can use DFS to find cycles if it sees an edge that leads to an already visited node, making sure to ignore the vertex it came from //
"Revise transitive closure, Floyd-Warshall's algorithm //Bellman-Ford algorithm solves single source, all targets problem for graphs with negative weights (returns error if it has negative cycles) // Can be done in O(VE) time // Dijkstra's: Make sure you insert the entry in discovered as v.distance = w // Bellman-Ford, need to draw table from 0...V-1 for each vertex WITH one more iteration to check for negative cycles//  Think of Floyd-Warshall's algorithm as the middleman-algorithm // For every middleman vertex k, for every pair of vertices update the distance of as the min(current distance OR going through middleman) // If floyd-warshall algorithm has a negative cycle will have vertex v such that dist[v][v] is negative // Floyd-Warshall and Transitive Closure both have complexities of O(V^3) time and O(V^2) space // Transitive closure is just a modification to floyd-warshall by preprocessing True between vertices that are already connected or vertices that are the same // 
"
"Union-Find has two operations SET_ID(u) which finds which set a vertex belongs to (O(1)) and UNION_SETS(Si, Sj) which combines two sets (O(V log V)) // Both Prims and Kruskals uses the same proof by contradiction in inductive step  // Prims is very similar to Dijkstra's except when you insert/update an entry, it's the weight of the edge NOT the weight + u.distance // Prim's also adds the edge to discovered, NOT just the vertex. When adding to finalized, just add the edge // Prim's Invariant: Finalized is a growing subset of a minimum spanning tree // Kruskal's algorithm: Finalized is a growing subset of a MST // Kruskal's is O(V log V) for union_sets as height of O(log V) with O(V) each // 
"
Flow network has 2 properties:flow is always less than capacity, flow in a vertex = flow out of a vertex // Flow of network is total flow out of source vertex // Ford-Fulkerson Algorithm works by 1) create residual network 2) find augmenting path 3) augment graph based on residual capacity 4) update residual network // Capacity of a cut is the total capacity of outgoing edges // Flow of cut is total outgoing - total incoming // Flow of the network = Flow of any cut // Max flow of a network is equal to min-cut and can be found using FF algorithm //
"Revise quicksort complexity, Bellman-Ford algorithm, transitive closure,recurrence relations // Dijkstra's alg does not work with graphs with negative weights (not cycles) // When discussing why recursive implementation for DP is bad -> Mention repeating calculations and provide example // Examples of DP algorithms: Floyd-Warshall algorithm, LCS, Bellman-Ford algorithm // Graph representation is either adjacency list or matrix // Want to use adjacency matrix for transitive closure if runtime is important and graph is not sparse // Quicksort is O(n log n) at worst when pivot is median, O(n^2) when pivot is smallest or largest in array // Quicksort average case: After partitioning, can either be in green subarray (worst case partition size of 1 and N-1) or grey subarray (worst case partition size of N/4 and 3/4N). If pivot is always in green will keep partitioning into 3/4 N and stop at where size is 1 so (3/4)^h N = 1 -> N = (4/3)^h -> h = log4/3(N), max possible height when pivot in green is 2 log4/3(N) so O(log N)  // Recurrence relations: Start with general expression and base case, find expressions for RHS terms and then sub into original equation, find pattern using k and try to get to base case, evaluate // Bellman-Ford basically relaxes all edges continually for V-1 times and then checks for any possible relaxations after that (if it does occur there is a negative cycle) // Transitive closure first checks True for every adjacent vertex and for the vertex itself, then checks for every vertex, and every pair of vertex to see if it can get to the other via the original vertex //
"
Revise complexity definitions, adjacency list, adjacency matrix operation complexities, BFS and DFS applications/data structure, recurrence relation calculations  // Counting sort and radix sort is done in linear time cos of the assumption that input is only integers in a small range // Can heapify an array in O(n) time so runtime for heapsort dominated by deletion operations // Dijkstra's algorithm is a greedy algorithm // Best-case input for sorting is when the list is already sorted // Heap elements should start at 1, not 0 // Floyd-Warshall is an all-pairs shortest path algorithm, use Dijkstra's or BF for single source // Dijkstra's preferred if no negative edges, or else use Bellman-Ford // Big-Theta is the lower bound // Complexity definition: Think of the graph of the OG function being bound or binding another function // DFS and BFS are both O(V+E) if using adjacency list // Adjacency matrix: bad space, can find neighbours in O(V), check edge in O(1) // Adjacency list : Space O(V+E), O(V) or O(log V) to check if edge exists, O(X) to check all adjacent vertices // Recurrence relations: Need consider other operations that may be O(N) and so on -> RUN THE RAM MODEL // Recurrence relation: Also need to consider that some operations may not occur in the base case that would normally occur in the recursive case //
-
Revise BWT, suffix trees, edit distance recurrence relation, heapify complexity, binary search invariant, Dijkstra's with negative weights, DFS for topsort,  BWT actual algorithm, LCS problem // Best case for insertion sort is O(n), // Invariant for binary search is key is in array[lo...hi) // When drawing DAG according to criteria, need to include every point // Can always just use DFS for top sort // Can iterate through graph by "for V = 1 to N in G" // Last-First Property: The last character in the row comes before first column // Can decompress BWT due to the fact that relative order of letters remain the same // BWT: Efficient method number each char in last column by relative order, row = 1 (want to start with character before $ which is always in first row), str = $, iterate until word is made -> {get letter based on row and add to str, row = rank[c] + num(c) - 1(basically offsetting downwards from the first occurence of that letter) // Edit distance: situations are letters matching, substituting s1[n] into s2[m], adding s2[m] into s1[n], removing s1[n] (ALL THE COMPROMISES ARE DONE TO FIRST STRING) // Edit distance base case: same as length of word (memo[i,0] = i;  memo[0,j] = j) // Best case of heap sort is when every element is the same -> no need to downheap // Suffix tree is O(n) space as the total number of leaf nodes is O(n) and each node has at least 2 children so complexity is O(n + n/2 + n/4) == O(n) //Heapify takes O(n) because nodes with a height of 1 need at worst O(1) operations and there are about N/4 of these nodes so O(N/4) + O(N/8) * 2  + ... + h * 1 which converges to N // Longest common subsequence: Recurrence relation is if letters match then its LCS(i-1, j-1) if they dont its max(LCS(i-1, j), LCS(i, j-1))

Revise network-flow topic, BWT query processing, Prim's/Kruskal's proof // In dense graphs, O(E) = O(V^2) // Ford Fulkerson Complexity is O(EF) where F is the max flow of the network. Each iteration of while loop is O(V+E) and this repeats O(F) times -> O(F(V+E)) -> O(EF) as O(E) >= O(V^2) for connected graphs // Unbounded knapsack is the single axes problem, 0/1 knapsack is the double axes problem // For proving induction step: Need to represent as a value k (NOT N) // For recurrence relations: When you have an expression like T(N/2) + aN, remember to substitute ALL N WITH N/2 WHEN FINDING T(N/2) // Suffix array: Must include "$" position in the array // Space complexity of building suffix array needs original string, rank array and suffix array which are all O(N) so O(N) in total // Bellman-Ford algorithm: Need to include and update pred // Space complexity of BF algorithm is O(V+E) // When FF terminates, there is a cut where both conditions are true //  Finding min-cut: Solve FF to find residual network with no augmenting paths then find vertices reachable from s (this forms cut-set S) // Only time you use adjacency matrix for graphs is during Floyd-Warshall algorithm or transitive closure // BWT Substring finding: Go backwards from string, find first and last occurence of letter, relate that to the first column (that is your range), repeat until string is finished or no match is found // BWT: Size of range after finding substring in BWT is the number of occurences // Prims: Need to show that after you add an edge to the graph, its still an MST // Kruskals: Need to show that between two disjoint sets, after you add an edge to join the two, it is still an MST //
"""

def parse():
    results = "results.txt"
    file = open(results,'w')
    myList = [elem.strip() for elem in STRING.split("//")]

    for elem in myList:
        file.write(elem + "\n")
    file.close()


parse()
